
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>OSv Blog</title>
  <meta name="author" content="Cloudius Systems">

  
  <meta name="description" content="While we relax and don&rsquo;t have to fix anything on our slides (really, they&rsquo;re all done), other speakers at the Linux Foundation&rsquo;s &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://osv.io/blog/blog/page/3">
  <link href="/blog/favicon.ico" rel="icon">
  <link href="/blog/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/blog/atom.xml" rel="alternate" title="OSv Blog" type="application/atom+xml">
  <script src="/blog/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/blog/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href='http://fonts.googleapis.com/css?family=Open+Sans:400,600,700' rel='stylesheet' type='text/css'>
<!--script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://osv.io/js/cycle.js"></script>
<script src="http://osv.io/js/js.js" language="javascript"></script-->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43975320-1', 'osv.io');
  ga('send', 'pageview');
</script>

  

</head>

<body   >
  <div class="wrap">
  <div class="header"> <a class="logo" href="http://osv.io"><img src="http://osv.io/images/logo.jpg" /> </a>
    <div class="topMenu">
        <ul class="nav menu">
            <li><a href="http://osv.io">Home</a></li>
            <li class=""><a href="/blog/">Blog</a></li>
            <li class=""><a href="/blog/blog/archives">Archives</a></li>
        </ul>
        <ul class="subscription" data-subscription="rss">
            <li><a href="/blog/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
            
        </ul>
        
        <form action="https://www.google.com/search" method="get">
            <fieldset role="search">
                <input type="hidden" name="q" value="site:osv.io/blog" />
                <input class="search" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
        </form>
        
    </div>
</div>
<div class="mobile header">
    <a class="logo" href="http://osv.io/">
        <img src="http://osv.io/images/mobileLogo.jpg"/>
    </a>

    <div class="mobile-topMenu">
        <a href="javascript:void(0)" class="mobileMenuLink">&nbsp;</a>
        <div class="theMobileMenu">
            <ul class="nav menu">
                <li class=""><a href="" >Home</a></li>
                <li class="deeper parent"><a href="/users" >Users</a>
                    <ul class="nav-child unstyled small">
                        
                    </ul>
                </li>
                <li class="deeper parent"><a href="/dev" >Development</a>
                    <ul class="nav-child unstyled small">
                        
                    </ul>
                </li>
                <li class="deeper parent"><a href="/community" >Community</a>
                    <ul class="nav-child unstyled small">
                        
                    </ul>
                </li>
                <li><a href="/contact/" >Contact</a></li>
            </ul>
        </div>
        <script>
            jQuery(document).ready(function(e) {
                $('.mobileMenuLink').click(function(e) {
                    $(this).toggleClass('active');
                    $('.theMobileMenu').slideToggle();
                });
            });
        </script>
    </div>
</div>
<div class="mobile mobilePageTitle"></div>


  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/08/20/linuxcon-cloudopen/">OSv in the Spotlight at LinuxCon/CloudOpen 2014</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-20T16:10:31-07:00" pubdate data-updated="true">Aug 20<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>While we relax and don&rsquo;t have to fix anything on our slides (really, they&rsquo;re all done), other speakers at the Linux Foundation&rsquo;s <a href="http://events.linuxfoundation.org/events/cloudopen-north-america">CloudOpen North America conference</a> in beautiful Chicago have some observations to make about OSv.</p>

<p><img src="/blog/images/chicago-river.jpeg" alt="Chicago River" /></p>

<p><strong>The Linux Foundation always picks great conference locations.</strong></p>

<p><a href="http://lccona14.sched.org/speaker/rcpavlicek">Russell Pavlicek</a> from the Xen project mentioned several library OSs that run on Xen in his <a href="http://lccona14.sched.org/event/17fdf31e5913cc4ebd5cf1f2ec039aa0">talk on new Xen features</a>.  He called the concept &ldquo;one of the biggest advances in the cloud.&rdquo;  Earlier library OSs have shown how much performance and simplicity gains are available, and OSv is extending the idea to ordinary POSIX and Java applications.</p>

<p><a href="http://lccona14.sched.org/speaker/mikeday">Mike Day</a> from IBM said, &ldquo;the engineers who write OSv are really good C++ coders,&rdquo; and called the project &ldquo;some of the finest C++ source code I&rsquo;ve ever seen&rdquo; in his <a href="http://lccona14.sched.org/event/434032efc316cc7aa98d4d590abda72e">talk on cloud operating systems for servers</a>.  He also had some praise for the <a href="http://osv.io/blog/blog/2014/04/19/spinlock-free/">spinlock-free way</a> that OSv handles mutexes, which as regular readers of this blog will know is important to prevent the dreaded lock-holder preemption problem.</p>

<p>If you&rsquo;re at LinuxCon, excuse me, #linuxcon, please come over and say &ldquo;hi&rdquo; to the OSv speakers: Don Marti and Glauber Costa. Hope to see you at the event, and please come to &ldquo;<a href="http://lccona14.sched.org/event/4684a80dd37f200277e971133920a2d0">Beating the Virtualization Tax for NoSQL Workloads With OSv</a>&rdquo; on Friday at 10:45 in the Colorado room.   Otherwise, you can get general OSv updates by subscribing to this blog&rsquo;s <a href="http://osv.io/blog/atom.xml">feed</a>, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>

<p><strong>Chicago River photo: <a href="http://commons.wikimedia.org/wiki/File:Chicago_river_2004.jpg">Urban for Wikimedia Commons</a></strong></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/08/14/redis-memonly/">Redis on OSv</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-14T09:26:31-07:00" pubdate data-updated="true">Aug 14<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>By Glauber Costa and Don Marti</strong></p>

<p>We&rsquo;re planning to attend the
Linux Foundation&rsquo;s <a href="http://events.linuxfoundation.org/events/cloudopen-north-america">CloudOpen North America conference</a>.  Hope to see you there, and please come to our talk, &ldquo;<a href="http://lccona14.sched.org/event/4684a80dd37f200277e971133920a2d0">Beating the Virtualization Tax for NoSQL Workloads With OSv</a>.&rdquo;</p>

<p>We&rsquo;ll be using a popular NoSQL database for our demo: Redis.  If you&rsquo;d like to follow along, you&rsquo;re welcome to clone and build Redis on OSv.  We&rsquo;re big Redis fans, because it&rsquo;s a fast, easy-to-administer, in-memory database that works with many useful data structures.</p>

<h2>Redis A to Z</h2>

<p><a href="http://redis.io/">Redis</a> is a remarkably useful piece of software.  People on the Internet talk about Redis a lot.  Here&rsquo;s what Google Suggest has to say about it: <strong>Atomic, benchmark, cluster, delete, expire, failover, gem, hash, incr, Java, key, list, master/slave, node.js, objects, Python, queue, Ruby, set, ttl, Ubuntu, &ldquo;vs. mongodb&rdquo;, Windows, XML, yum, zadd</strong>.  (<a href="http://redis.io/commands/ZADD">zadd</a> is a really cool command by the way.  A huge time-saver for maintaining &ldquo;scoreboard&rdquo; state for games and content-scoring applications.  Did we mention that we&rsquo;re Redis fans?)</p>

<p>Redis fills a valuable niche between memcached and a full-scale NoSQL database such as Cassandra.  Although it&rsquo;s fast and perfectly usable as a simple key-value store, you can also use Redis to manage more featureful data structures such as sets and queues.</p>

<p>It makes a great session cache, lightweight task queue, or a place to keep pre-rendered content or ephemeral data, and it&rsquo;s a <a href="http://highscalability.com/display/Search?moduleId=4876569&amp;searchQuery=redis">star at highscalability.com</a>.</p>

<p>But you probably already know that.</p>

<h2>Building Redis on OSv</h2>

<p>Redis works normally on OSv except for one feature: the
<a href="http://redis.io/commands/bgsave">BGSAVE</a>
command.  A Redis background
save depends on the operating system&rsquo;s
<a href="http://en.wikipedia.org/wiki/Copy-on-write#Copy-on-write_in_virtual_memory_management">copy-on-write</a>
functionity.  When you issue the BGSAVE
command, the parent Redis process calls
<a href="http://en.wikipedia.org/wiki/Fork_%28system_call%29">fork</a>,
and the parent process keeps running while the child
process saves the database state.</p>

<p>Copy-on-write ensures that the child process sees
a consistent set of data, while the parent gets its
own copy of any page that it modifies.</p>

<p>Because OSv has a single address space,
that isn&rsquo;t an option here. OSv support Redis
<a href="http://redis.io/commands/save">SAVE</a> but not BGSAVE. Other than that,
running redis on OSv requires little effort. From the OSv source tree,
all one should do is:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>make image=redis-memonly</span></code></pre></td></tr></table></div></figure>


<p>Now you have a <code>usr.img</code> file, which you can run locally with OSv&rsquo;s <code>run.py</code>.  Behind the scenes, all that this build step is doing is to issue the application&rsquo;s <code>make</code>, with the right set of flags so redis is a shared library.  For more info on how to do that, see <a href="http://osv.io/blog/blog/2014/04/03/capstan/">our earlier example</a>.  of how to use the <code>-fPIC</code> and <code>-shared</code> options.</p>

<h2>Is it fast?</h2>

<p>We have been running redis on local machines to test many of its
functionalities and help us mature OSv.  As with any piece of software, the
result of course depends on many factors.  Because OSv is an operating system
designed for the cloud, we wanted to showcase its performance running on Amazon
EC2.</p>

<p>To do that, we have selected the <a href="http://aws.amazon.com/ec2/instance-types/">c3.x8large</a> machines.  They feature 32 CPUs and
60Gb of memory each. We are fully aware this is an overkill in the case
of Redis &ndash; a single threaded application. However, those are the only machines
that Amazon advertises as featuring 10Gb networking, and we didn&rsquo;t want the
network to be a bottleneck for the sake of the benchmark. Also, smaller
machines cannot be put in EC2 placement groups. It all boils down to the network!</p>

<p>So in this benchmark, no more than two cores should be active at any given time &ndash; one for redis, one for network interrupt processing. In a real scenario, one could easily deploy in a smaller machine.</p>

<h3>Benchmark setup</h3>

<p>We have benchmarked redis&#8217; latest beta (beta-8) running both on OSv, and on an
Ubuntu14 AMI. To do that, we have just launched a new AMI, selected
Ubuntu14.04, and launched it. Once it launched, we have downloaded and compiled
redis&#8217; latest, and moved the redis.conf used by OSv to the machine. The only
difference in that configuration file from what is shipped with redis by
default, is that we disable disk activity. As already explained,
OSv currently do not support that, and to be fair, the Linux guest we are
comparing against should not hit the disk either at any point.</p>

<p>On ubuntu, redis was run with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>numactl --physcpubind=1 redis-server ~/redis.conf</span></code></pre></td></tr></table></div></figure>


<p>Using numactl considerably reduces the standard deviation coming from the Linux
scheduler moving the thread around.</p>

<p>The <code>redis-benchmark</code> command was issued in another machine of the same type,
running in the same zone and placement group.</p>

<p>The two commands were:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>numactl --physcpubind=1 redis-benchmark --csv -h &lt;IP&gt; -c 50 -n 100000 -P 1</span></code></pre></td></tr></table></div></figure>


<p>and later on, to demonstrate how OSv can handle larger messages,</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>numactl --physcpubind=1 redis-benchmark --csv -h &lt;IP&gt; -c 50 -n 100000 -P 16</span></code></pre></td></tr></table></div></figure>


<p>What this last command does, is to exercise redis&#8217; <code>pipeline</code> feature, that
can send multiple &ndash; in this case 16 &ndash; commands in the same packet. This will
decrease the impact of the round trip time in the final figure.</p>

<p>The difference can be clearly seen in the graph&hellip;</p>

<p><a href="/blog/images/redis.png"><img src="/blog/images/redis.png" alt="Redis benchmark results" /></a></p>

<p>Note that the LRANGE class of commands has a significantly different pattern
than the other commands. In that command, the client sends a very short query,
and receive a potentially very large reply, thereby exercising the transmission
path, rather than the receive path of OSv. This table shows that our transmission
path is lacking a bit of love, particularly when the response sizes grows (as the
pipeline level increases)</p>

<h2>Conclusions</h2>

<p>OSv is a fast maturing, but not yet mature operating system, soon to be in beta
phase. We have gaps to close, as can be seen in the case of LRANGE set of
benchmarks. So far, we have focused our efforts in technologies around the
receive path, and it has paid off: We can offer a level of performance far
beyond what an out of the box distribution can. Some features that we
architecturally lack, makes the use of Redis as a full-blown on-disk database
challenging. But if you want to serve your load from memory, the OSv promise
delivers: With OSv, you don&rsquo;t have to pay the virtualization tax.</p>

<p>If you&rsquo;ll be at CloudOpen, you can <a href="http://lccona14.sched.org/event/4684a80dd37f200277e971133920a2d0">add our talk to your schedule now</a>.</p>

<p>If you have any questions on running Redis or any other application, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.  You can get general updates by subscribing to this blog&rsquo;s <a href="http://osv.io/blog/atom.xml">feed</a>, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/07/27/capstan-lein-template/">Running Clojure on OSv: Easier With a New Capstan Template</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-27T00:00:00-07:00" pubdate data-updated="true">Jul 27<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Clojure developers usually do not care too much about the underlying OS.
The low-level differences between Linux, Mac OS, and even Microsoft Windows are abstracted away by the JVM.</p>

<p>When deploying Clojure code on the cloud, there used to be one default choice &ndash; Linux.
But Linux
<a href="http://osv.io/blog/blog/2014/07/21/generic-os-is-dead/">is not an ideal OS</a>
for pure cloud services.</p>

<p><a href="https://github.com/cloudius-systems/osv">OSv</a> is a new, open source OS, designed specifically for the cloud.  Since OSv supports the standard JVM, it is ideal for running Clojure applications on the cloud.  And the same configuration applies to building VMs for any cloud: public clouds such as Amazon&rsquo;s and Google&rsquo;s, private clouds based on VMware or KVM, or public and private OpenStack.</p>

<p>Porting a Clojure application to OSv was already
<a href="http://osv.io/blog/blog/2014/04/22/riemann-on-osv/">pretty easy</a>, but
now it&rsquo;s even easier.  This blog post describes a new <a href="https://github.com/tzach/capstan-lein-plugin">lein template</a> for OSv.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/blog/2014/07/27/capstan-lein-template/">Read more&#8230;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/07/21/generic-os-is-dead/">If Java Application Servers Are Dead, So Is the Operating System (in the Cloud)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-21T00:00:00-07:00" pubdate data-updated="true">Jul 21<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>By Tzach Livyatan</strong>
This post is a response to the excellent presentation <a href="http://www.slideshare.net/ewolff/java-application-servers-are-dead"><strong>&ldquo;Java Application Servers Are Dead!&rdquo;</strong></a> by <a href="https://twitter.com/ewolff">Eberhard Wolff</a>.
Go read his slides and come back here.</p>

<p>Back already?
Assuming you agree with Eberhard’s  claims,  let me demonstrate how
most of his points on Java Application Servers can be applied to a
generic OS (one designed for hardware servers) in the cloud as well.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/blog/2014/07/21/generic-os-is-dead/">Read more&#8230;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/07/14/osv-shrinker-api/">Getting Started With the OSv Shrinker API</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-14T08:37:05-07:00" pubdate data-updated="true">Jul 14<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>By Don Marti</strong></p>

<p>If you&rsquo;re writing a program that keeps a cache in memory, you&rsquo;re probably expecting users to have to set the cache size, which means a configuration setting or command-line argument.  And every configuration setting or command-line argument is something that you have to document, or explain to users when they get it wrong.</p>

<p>Thankfully, there&rsquo;s an easier way.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/blog/2014/07/14/osv-shrinker-api/">Read more&#8230;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/07/11/osv-boot/">Inside the OSv Boot Process</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-11T08:37:05-07:00" pubdate data-updated="true">Jul 11<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>What happens in that critical fraction of a second between when a virtual x86_64 system &ldquo;powers up&rdquo; and when it begins running operating system code written in a high-level language?</p>

<p>For a helpful walk-through, there&rsquo;s a new article on the OSv wiki, originally written by Elazar Leibovich.</p>

<p><img src="/blog/images/pc-power.jpeg" alt="IBM PC power switch" /></p>

<p><strong>The boot process for a modern VM traces its history back to the
original IBM PC.</strong></p>

<p>While some of the intricate startup steps are historic, the end result is an OSv boot time that&rsquo;s less than a second&mdash;an order of magnitude faster than a conventional multi-user OS.  To read (or contribute!) details,
<a href="http://github.com/cloudius-systems/osv/wiki/OSv-early-boot-(MBR)">the &ldquo;OSv early boot&rdquo; article on the wiki.</a></p>

<p>(photo: <a href="http://commons.wikimedia.org/wiki/File:XT-PC-Power-Supply-PCB-IMG_0436.JPG">Hans Haase for Wikimedia Commons</a>. Available under the Creative Commons Attribution-Share Alike 3.0 Unported license.)</p>

<p>If you have any questions on OSv internals, or porting your application, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.  You can get general updates by subscribing to this blog&rsquo;s feed, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/07/01/s3stat/">What Is the Most Popular OSv Virtual Appliance?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-01T00:00:00-07:00" pubdate data-updated="true">Jul 1<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>By Tzach Livyatan</strong></p>

<p>(Spoiler: It&rsquo;s Apache Tomcat.)</p>

<p><a href="https://github.com/cloudius-systems/capstan">Capstan</a> is a tool for rapidly building and running applications on OSv.
As with Docker, Capstan users can download and run images from a public repository.
We chose to implement our public Capstan repository using <a href="http://aws.amazon.com/s3/">Amazon S3</a>.</p>

<p>Amazon S3  gives us the flexibility and security we need, but by default it&rsquo;s missing a critical feature: download statistics.
This statistics are very interesting to us, to evaluate which of the Capstan virtual appliances are more popular.  Fortunately, there is an easy way to gather the stats we need.</p>

<p>After a short tools survey, we choose <a href="http://www.s3stat.com/">s3stat</a>.</p>

<p><a href="http://www.s3stat.com/">s3stat</a> is a cloud-based service which can follow an S3 bucket, and visualize download statistics, by file, country, browser day, or otherwise.
The price makes sense, and it is super easy to enable.</p>

<p><img src="/blog/images/s3stat_chart.png" alt="s3stat chart" /></p>

<p>So what are the results? (drum roll&hellip;.)</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/blog/2014/07/01/s3stat/">Read more&#8230;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/06/23/containers-hypervisors-part-3/">Hypervisors Are Dead, Long Live the Hypervisor (Part 3)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-06-23T13:00:00-07:00" pubdate data-updated="true">Jun 23<span>rd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>By Dor Laor and Avi Kivity</strong></p>

<h1>The new school: functionality, isolation and simplicity</h1>

<p>(This is part 3 of a 3-part series. <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/">Part 1</a>, <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/">Part 2</a>)</p>

<p>Containers make administration simple, and VMs give you portability, isolation, and administration advantages. The concept of putting <a href="http://www.slideshare.net/jpetazzo/linux-containers-lxc-docker-and-security">containers inside VMs</a> gives you the isolation you need, but there are now two layers of configuration and overhead instead of one.</p>

<p>What if there were one technology that could give us the simplicity and reduced overhead of containers and the security, tools, and hardware support of hypervisors? That’s where OSv comes in.  OSv, the “Operating System Designed for the Cloud.” takes an approach different from either containerization or virtualizing an existing bare-metal OS.  OSv is a single address space OS, designed to run as a guest only, with one application per VM.</p>

<h2>Best of both worlds?</h2>

<p>Glauber Costa, in a speech at Linuxcon called <a href="https://events.linuxfoundation.org/images/stories/pdf/lceu2012_costa.odp">&ldquo;The failure of Operating Systems, and how we can fix it&rdquo;</a>, pointed out that the existence of hypervisors is evidence that Operating Systems alone cannot meet some of the demands of real workloads. Through OSv, we have the opportunity to work together with the hypervisor to create a superior solution to what can be done with the OS alone: combining the resource efficiency of containers with the processor-aided advantages of hardware virtualization.</p>

<p>In the eight years since the release of Intel VMX, the silicon has kept getting better and better at moving the costs of virtualization into hardware. Enterprise customers have been demanding lower virtualization overhead for as long as hypervisors have been a thing, and the best minds of the CPU industry are working on it. With nested page tables and other features coming “for free” on the processor, virtualization overhead is being squeezed closer and closer to parity with bare metal.</p>

<p><img src="/blog/images/duplication.png" alt="typical cloud stack with duplication" /></p>

<p><strong>Typical cloud stacks have duplicate functionality at the hypervisor, guest OS, and application levels.</strong></p>

<p>While many players are trying to carve out a simple OS containerization system at the guest OS level, they are ignoring the stable, simple, secure, hardware-supported interface we already have: the hypervisor-guest interface.  There’s nothing that says we have to use this well-tested, industry-standard interface just to run a large, complete OS designed for bare metal. (In fact, research projects such as “Erlang on Xen” and MirageOS have explored using the hypervisor to run something less than a full OS for quite a while.)</p>

<h2>OSv is designed to perform</h2>

<p>OSv transparently loads an application into its kernel space. There is no userspace whatsoever. It removes the need for user to kernel context switches. In addition, the kernel trusts the application, since it relies on the underlying hypervisor for isolation from other applications in other VMs. Thus it opens up a way for the application to use any kernel API – from taking scheduling decisions to zero copy operations on data, and even unlock the brute force of the hardware page tables for the benefit of the application or its framework.</p>

<p>To date (June 2014), OSv provides 4x better performance for Memcache, a 40% gain with Apache Tomcat, and a 20% gain with Cassandra and SPECjbb. These results are based on our alpha versions, and are likely to improve as we complete the optimizations remaining on our roadmap.</p>

<p><img src="/blog/images/workloads.png" alt="OSv example workloads" /></p>

<p><strong>OSv runs many key cloud workloads with low overhead and high performance.</strong></p>

<p>OSv&rsquo;s image is your app and our kernel. Sometimes it means an image size of 10MB! That&rsquo;s a 100-400x better than the traditional OS and resembles a container&rsquo;s footprint. The OSv boot time is under a second, which is also closer to container startup time.</p>

<h2>OSv management: some questions for devops</h2>

<p>How many configuration files does your OS have? <strong>OSv has zero.</strong></p>

<p>How many times have you had to perform string manipulation on UNIX-like config files? <strong>OSv is built for automation and uses a RESTful API instead.</strong></p>

<p>How hard is it to upgrade your OS, and how can you revert it? <strong>OS is stateless.</strong></p>

<p>With an hypervisor below, you get the features such as live migration, perfect SLA, superior security for free while you get to enjoy from OSv’s added value.</p>

<h2>Capstan – or what we have learned from Docker</h2>

<p>We do love Docker with regard to development. The neat public image repository and the dead-simple-single execution won our hearts. We wanted to have the same for VMs, so we created the <a href="http://osv.io/capstan/">Capstan</a> project. Capstan has a public image repository, and by executing &lsquo;capstan run cloudius/osv-cassandra&rsquo; a virtual machine image will be either downloaded to your laptop (Mac OS X, Microsoft Windows, or Linux) or be executed on your cloud of choice. Capstan also allows you to build images including an app and a base OSv image. It takes about three seconds. On Capstan&rsquo;s roadmap, we plan to support the Docker file format, run Java apps directly without a config file, and form a simple PaaS for developers to load their favorite app directly to a running VM.</p>

<h2>Pick a cloud, any cloud</h2>

<p>The business case for cloud computing has never been better for the customer. While Amazon continues to upgrade the available instances and offer faster VMs at lower prices, Google is coming on strong as well. Microsoft, HP, IBM, and others are all competing for cloud business.  The cloud VM is the new generic PC.  Because we can create standard VMs that will run on anyone’s cloud, or on a private or hybrid cloud, we can develop with the confidence that we’ll be able to deploy to whatever infrastructure makes business sense&mdash;or move, or split deployment.</p>

<p>Lastly, we like to point out we are not against containers. Container technology is awesome when used for the right scenario. As there are cases for public transportation versus private cars, the same applies to devops. Both containers and OSv excel, in different domains. Here is a simple flow chart that can guide you with your choices:</p>

<p><a href="/blog/images/flowchart.png"><img src="/blog/images/flowchart.png" alt="Guest OS selection flowchart" /></a></p>

<p>Using OSv on ubiquitous, secure, full-featured hypervisors is the way to keep performance up, costs down, and options open. We had to completely reinvent the guest OS to do it&mdash;but now that we have it, OSv is available to build on. Please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a> for technical info, or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems on Twitter</a> for the latest news.</p>

<p>( <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/">Part 1</a>, <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/">Part 2</a>
)</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/06/19/containers-hypervisors-part-2/">Hypervisors Are Dead, Long Live the Hypervisor (Part 2)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-06-19T13:00:00-07:00" pubdate data-updated="true">Jun 19<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>By Dor Laor and Avi Kivity</strong></p>

<h1>Linux containers</h1>

<p>(This is part 2 of a 3-part series. <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/">Part 1 was published yesterday.</a>)</p>

<p>Containers, which create isolated compartments at the operating system level instead of adding a separate hypervisor level, trace their history not to mainframe days, but to Unix systems.</p>

<p>FreeBSD introduced “jails” in 2000. There’s a good description of them in  <a href="http://phk.freebsd.dk/pubs/sane2000-jail.pdf">Jails: Confining the omnipotent root by Poul-Henning Kamp and Robert N. M. Watson</a>. Solaris got its Zones in 2005.  Both systems allowed for an isolated “root” user and root filesystem.</p>

<p>The containers we know today, <a href="https://linuxcontainers.org/">Linux Containers</a>, or LXC, are not a single monolithic system, but more of a concept, based on a combination of  several different isolation mechanisms built into Linux at the kernel level.  <a href="https://lwn.net/Articles/587545/">Linux Containers 1.0 was released earlier this year.</a>, but many of the underlying systems have been under development in Linux independently.  Containers are not an all-or-nothing design decision, and it’s possible for different systems to work with them in different ways. <a href="https://linuxcontainers.org/">LXC can use all of the following Linux features</a>:</p>

<ul>
<li><p>Kernel namespaces (ipc, uts, mount, pid, network and user)</p></li>
<li><p>AppArmor and SELinux profiles</p></li>
<li><p>Seccomp policies</p></li>
<li><p>Chroots (using pivot_root)</p></li>
<li><p>Kernel capabilities</p></li>
<li><p>Control groups (cgroups)</p></li>
</ul>


<p>Although the combination can be complex, there are tools that make containers simple to use. For several years userspace tools such as LXC, libvirt allowed users to manage containers. However, containers didn’t really get picked up by the masses until the creation of <a href="https://www.docker.io/">Docker</a>. Docker and <a href="http://man7.org/linux/man-pages/man1/systemd-nspawn.1.html">systemd-nspawn</a> can start containers with minimal configuration, or from the command line. The Docker developers deserve much credit for adding two powerful concepts above the underlying container complexity:
a. Public image repository &ndash; immediate search and download of containers pre-loaded with dependencies, and
b. Dead-simple execution &ndash; a one-liner command for running a container.</p>

<p><img src="/blog/images/docker.png" alt="Docker diagram" /></p>

<p><strong><a href="https://www.docker.io/the_whole_story/">Docker gives container users a simple build process and a public repository system.</a></strong></p>

<h2>Container advantages</h2>

<p>When deployed on a physical machine, containers can eliminate the need of running two operating systems, one on top of the other (as in traditional virtualization). It makes IO system calls almost native and the footprint is minimal. However, this comes with a cost as we will detail below. The rule of the thumb is that if you do not need multi-tenancy and you’re willing to do without a bunch of software defined features, containers on bare metal are perfect for you!</p>

<p>In production, <a href="https://speakerdeck.com/jbeda/containers-at-scale">Google uses containers extensively</a>, starting more than two billion per week. Each container includes an application, built together with its dependencies, and containerization helps the company manage diverse applications across many servers.</p>

<p>Containers are an excellent case for development and test. It becomes possible to test some fairly complex setups, such as a version control system with hooks, or an SMTP server with spam filters,  by running services in a container. Because a container can use namespaces to get a full set of port numbers, it’s easy to run multiple complex tests at a time. The systemd project even uses containers for testing their software, which manages an entire Linux system. Containers are highly useful for testing because of their fast startup time&mdash;you’re just invoking an isolated set of processes on an existing kernel, not booting an entire guest OS.</p>

<p>If you run multiple applications that depend on different versions of a dependency, then deploying each application within its own container can allow you to avoid dependency conflict problems. Containers in theory decouple the application from the operating system. We use the term &lsquo;in theory&rsquo; because lots of care and thought should be given to maintaining your stack. For example, will your container combo be supported by the host OS vendor? Is your container up-to-date and does it include fixes for bugs such as ‘heartbleed’? Is your host fully updated, and does its kernel API provide the capabilities your application requires?</p>

<p>We highly recommend the use of containers whenever your environment is homogeneous:</p>

<ul>
<li><p>No multitenancy</p></li>
<li><p>Application is always written with clustering in mind</p></li>
<li><p>Load balancing is achieved by killing running apps and re-spinning them elsewhere (as opposed to live migration)</p></li>
<li><p>No need to run different kernel versions</p></li>
<li><p>No underlying hypervisor (otherwise, you&rsquo;re just adding a layer)</p></li>
</ul>


<p>When the above apply, you will enjoy near bare-metal performance, a small footprint and fast boot time.</p>

<h2>Container disadvantages: Security</h2>

<p>It’s clear that a public cloud needs strong isolation separating tenant systems. All that an attacker needs is an email address and a credit card number to set up a hostile VM on the same hardware as yours. But strong isolation is also needed in private clouds behind the corporate firewall. Corporate IT won’t be keen to run <strong>sandbox42.interns.engr.example.com</strong> and <strong>payroll.example.com</strong> within the same security domain.</p>

<p>Hypervisors have a relatively simple security model. The interface between the guest and the hypervisor is well defined, based on real or virtual hardware specifications. Five decades of hypervisor development have helped to form a stable and mature interface. Large portions of the interface&rsquo;s security are enforced by the physical hardware.</p>

<p>Containers, on the other hand, are implemented purely in software. All containers and their host share the same kernel. Nearly the entire Linux kernel had to undergo changes in order to implement isolation for resources such as memory management, network stack, I/O, the scheduler, and user namespaces. The Linux community is investing a lot of effort to improve and expand container support. However, rapid development makes it harder to stabilize and harden the container interfaces.</p>

<h2>Container disadvantages: Software-defined data center</h2>

<p>Hypervisors are the basis for the new virtualized data center. They allow us to perfectly abstract the hardware and play nicely with networking and storage.
Today there isn&rsquo;t a switch or a storage system without VM integration or VM-specific features.</p>

<p>Can a virtualized data center be based on containers in place of hypervisors? At almost all companies, no.. There will always be security issues with mounting SAN devices and filesystems from containers in different security domains. Yes, containers are a good fit for plenty of tasks but are restricted when it comes to sensitive areas such as you data center building blocks such as the storage and the network.</p>

<p>No one operating system, even Linux, will run 100% of the applications in the data center. There will always be diversity at the data center, and the existence of different operating systems will force the enterprise to keep the abstraction at the VM level.</p>

<h2>Container disadvantages: Management</h2>

<p>The long history of hypervisors means that the industry has developed a huge collection of tools for real-world administration needs.</p>

<p><img src="/blog/images/esx2.jpg" alt="CPM monitoring in VMware" /></p>

<p><strong><a href="http://robertmorannet.blogspot.com/2010/08/vmware-vsphere-screenshots.html">Blogger Robert Moran shows a screenshot of CPU monitoring in VMware’s vSphere</a>.</strong></p>

<p>The underlying functionality for hypervisor management is also richer. All of the common hypervisors support “live migration” of guests from one host to another.</p>

<p>Hypervisors have become an essential tool in the community of practice around server administration. Corporate IT is in the process of virtualizing its diverse collection of servers, running modern and vintage Linux distributions, plus legacy operating systems, and hypervisor vendors including VMWare and Microsoft are enabling it.</p>

<h2>Container disadvantages: Complexity</h2>

<p>While containers take advantage of the power built into Linux, they share Linux’s complexity and diversity.  For example, each Linux distribution standardizes on a different kernel version, and some use AppArmor while others use SELinux. Because containers are implemented using multiple isolation features at the OS level, the “containerization” features can vary by kernel version and platform.</p>

<h2>The anatomy of a multi-tenant exploit</h2>

<p>Let&rsquo;s assume a cloud vendor, whether SaaS, IaaS, or PaaS, implements a service within a container. How would an attacker exploit it?
The first stage would be to gain control of the application within the container. Many applications have flaws and the attacker would need to exploit an existing unpatched CVE in order to gain access to the container. IaaS even makes it simpler as the attacker already has a “root” shell inside a neighboring container.</p>

<p>The next stage would be to penetrate the kernel. Unfortunately, the kernel&rsquo;s attack surface contains hundreds of system calls, and other vulnerabilities exist in the form of packets and file metadata that can jeopardize the kernel. Many attackers have access to zero-day exploits, unpublished local kernel vulnerabilities. (A typical “workflow” is to watch upstream kernel development for security-sensitive fixes, and figure out how to exploit them on the older kernels in production use.)</p>

<p>Once the hacker gains control of the kernel, it&rsquo;s game over. All the other tenants’ data is exposed.</p>

<p>The list of exploitable bugs is always changing, and there will probably be more available by the time you read this.  A few recent examples:</p>

<ul>
<li><p>“An information leak was discovered in the Linux kernel&rsquo;s SIOCWANDEV ioctl call. A local user with the CAP_NET_ADMIN capability could exploit this flaw to obtain potentially sensitive information from kernel memory.“ (CVE-2014-1444) Some container configurations have CAP_NET_ADMIN, while others don’t. Because it’s possible to set up containers in more or less restricted ways, individual sites need to check if they’re vulnerable. (Many LInux capabilities are <a href="http://forums.grsecurity.net/viewtopic.php?f=7&amp;t=2522">equivalent to root</a> because they can be used to obtain root access.)</p></li>
<li><p>“An information leak was discovered in the wanxl ioctl function in  Linux. A local user could exploit this flaw to obtain potentially sensitive information from kernel memory.” (CVE-2014-1445)”</p></li>
<li><p>“An unprivileged local user with access to a CIFS share could use this flaw to crash the system or leak kernel memory. Privilege escalation cannot be ruled out (since memory corruption is involved), but is unlikely.“ (CVE-2014-0069)</p></li>
</ul>


<p>Each individual vulnerability is usually fixed quickly, but there’s a constant flow of new ones for attackers to use. <a href="https://lwn.net/Articles/462756/">Linux filesystem developer Ted Ts’o wrote</a>,</p>

<blockquote><p>Something which is baked in my world view of containers (which I suspect is not shared by other people who are interested in using containers) is that given that the kernel is shared, trying to use containers to provide better security isolation between mutually suspicious users is hopeless.  That is, it&rsquo;s pretty much impossible to prevent a user from finding one or more zero day local privilege escalation bugs that will allow a user to break root.  And at that point, they will be able to penetrate the kernel, and from there, break security of other processes.</p>

<p>So if you want that kind of security isolation, you shouldn&rsquo;t be using containers in the first place.  You should be using KVM or Xen, and then only after spending a huge amount of effort fuzz testing the KVM/Xen paravirtualization interfaces.</p></blockquote>

<p><a href="http://permalink.gmane.org/gmane.linux.coreos.devel/287">Kernel developer Greg Kroah-Hartman wrote</a>,</p>

<blockquote><p>Containers are not necessarily a &ldquo;security&rdquo; boundary, there are many &ldquo;leaks&rdquo; across it, and you should use it only as a way to logically partition off users/processes in a way that makes it easier to manage and maintain complex systems. The container model is quite powerful and tools like docker and systemd-nspawn provide a way to run multiple &ldquo;images&rdquo; at once in a very nice way.</p></blockquote>

<p>Containers are powerful tools for Linux administrators, but for true multi-tenant cloud installations, we need stricter isolation between tenants.</p>

<p>Containerization is not “free”. For instance, the Linux Memory Controller can slow down the kernel by as much as 15%, just by being enabled, with no users. The Memory Controller itself is complicated, but cgroups controllers, on which it depends, are also complex. The surface of change is just way too big, and the resulting implementation necessarily too complex. <a href="https://plus.google.com/109487070944143253361/posts/6AdwyTfPFQe">George Dunlap said it best</a>,</p>

<blockquote><p>With containers you&rsquo;re starting with everything open and then going around trying to close all the holes; if you miss even a single one, bam, you lose. With VMs, you start with almost everything closed, and selectively open things up; that makes a big difference.</p></blockquote>

<p><strong>This is part 2 of a 3-part series.</strong> Please subscribe to our <a href="/blog/atom.xml">feed</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> to get a notification when part 3 is available.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/blog/2014/06/19/containers-hypervisors-part-1/">Hypervisors Are Dead, Long Live the Hypervisor (Part 1)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-06-19T13:00:00-07:00" pubdate data-updated="true">Jun 19<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>By Dor Laor and Avi Kivity</strong></p>

<p>The hypervisor is the basic building block of cloud computing; hypervisors drive the software-defined data center revolution, and two-thirds of all new servers are virtualized today. Hypervisors for commodity hardware have been the key enabler for the software revolution we have been experiencing.</p>

<p>However, for the past 8 years a parallel technology has been growing, namely, containers. Recently containers have been getting a fairly large amount of traction with the development of the Docker project. When run on bare metal, containers perform better than hypervisors and have a lower footprint.</p>

<p>There is a lot in common with the goals of these technologies. These three blog entries will try to provide an answer to the question</p>

<p><strong>Will containers kill the hypervisor?</strong></p>

<p>The series will provide in-depth explanations about the underlying technology and the pros and cons of each solution.</p>

<h2>Intro: ancient hypervisor lore</h2>

<p>What is virtualization anyway?</p>

<p><img src="/blog/images/virtualization.png" alt="Virtualization diagram" /></p>

<p>A hypervisor is a software component (potentially assisted by hardware) that allows us to run multiple operating systems on the same physical machine. The overlay OS is called the guest OS or simply, a Virtual Machine (VM). The guest OS may not even be aware it is running on virtual hardware.</p>

<p>The interface between the guest and the host is the hardware specification. It covers the CPU itself and any other hardware devices, from BIOS to NICs, SCSI adapters, GPUs and memory.</p>

<p><img src="/blog/images/IBM360-67AtUmichWithMikeAlexander.jpg" alt="IBM System/360" /></p>

<p><strong>IBM, together with MIT and the University of Michigan, pioneered hypervisor technology on System/360 and System/370 mainframes, beginning in the 1960s.</strong></p>

<p>IBM was the first company to produce hypervisors. The IBM System/360, model 1967, was the first to <a href="http://www.beagle-ears.com/lars/engineer/comphist/ibm360.htm#gener">ship with virtual memory hardware supporting virtualization</a>. The next system in the series, System/370, was the “private cloud” of its day. Administrators could set up virtual machines for running different OS versions, and even public-cloud-like “time sharing” by multiple customers.</p>

<h2>Virtualization for x86</h2>

<p>Virtualization didn’t make it to commodity systems until the <a href="http://www.vmware.com/company/news/mediaresource/milestones">release of VMware Workstation in 1999</a>. In the early 2000s, hypervisors were based on pure software and were mostly useful for development and testing.  VMware initially used a technique called dynamic translation to intercept privileged operations by the guest operating system. When the guest accessed “hardware”, VMWare rewrote the instructions on the fly, to protect itself from the guest and isolate guests from each other.</p>

<p><img src="/blog/images/vmware-logo.png" alt="VMware logo" /></p>

<p>Later on, the open source Xen hypervisor project coined the term paravirtualization (PV). PV guests, which have to be specially modified to run on a PV host, do not execute privileged instructions themselves but ask the hypervisor to do it on their behalf.</p>

<p><img src="/blog/images/xen-logo.png" alt="Xen logo" /></p>

<p>Eventually, Intel, AMD and ARM implemented support for virtualization extensions. A special host mode allows running guest code on the bare CPU, getting near 100% of bare metal throughput for CPU-intensive workloads. In parallel, the memory management and the IO path received attention as well with technologies such as nested paging (virtual memory), virtual interrupt controllers, single-root I/O virtualization (SRIOV) and other optimizations.</p>

<h2>Hardware support for hypervisors</h2>

<p>Hypervisor enablement continues to be a priority for hardware manufacturers. <a href="https://plus.google.com/+OsvIo/posts/fgzsepcScTa">Glauber Costa wrote</a>, “the silicon keeps getting better and better at taking complexity away from software and hiding somewhere else.”</p>

<p><a href="http://www.redhat.com/rhecm/rest-rhecm/jcr/repository/collaboration/jcr:system/jcr:versionStorage/5e7884ed7f00000102c317385572f1b1/1/jcr:frozenNode/rh:pdfFile.pdf">According to a paper from Red Hat Software</a>,</p>

<blockquote><p>Both Intel and AMD continue to add new features to hardware to improve performance for virtualization. In so doing, they offload more features from the hypervisor into “the silicon” to provide improved performance and a more robust platform&hellip;.These features allow virtual machines to achieve the same I/O performance as bare metal systems.</p></blockquote>

<h2>Old-school hypervisors</h2>

<p>Hypervisors are one of the main pillars of the IT market (try making your way through downtown San Francisco during VMworld) and solve an important piece of the problem. Today the hypervisor layer is commoditized, users can choose any hypervisor they wish when they deploy Open Stack or similar solutions.</p>

<p>Hypervisors are a mature technology with a rich set of tools and features ranging from live migration, cpu hotplug, software defined networking and other new coined terms that describe the virtualization of the data center.</p>

<p>However, in order to virtualize your workload, one must deploy a full fledged guest operating system onto every VM instance. This new layer is a burden in terms of management and in terms of performance overhead. We’ll look at one of the other approaches to compartmentalization next time: containers.</p>

<p><strong>This is part 1 of a 3-part series.</strong>  <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/">Part 2 is now available</a>.  Please subscribe to our <a href="/blog/atom.xml">feed</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> to get a notification of future posts.</p>

<p>Photo credit, IBM 360: <a href="http://commons.wikimedia.org/wiki/File:IBM360-67AtUmichWithMikeAlexander.jpg">Dave Mills for Wikimedia Commons</a></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/blog/page/4/">&larr; Older</a>
    
    <a href="/blog/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/blog/page/2/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/blog/2014/12/04/spamsink/">Making Spam Vanish&hellip;in the CLOUD</a>
      </li>
    
      <li class="post">
        <a href="/blog/blog/2014/12/02/docker-rocket/">Containers, Containers, Containers! More Options for the Cloud</a>
      </li>
    
      <li class="post">
        <a href="/blog/blog/2014/11/17/eblug/">OSv Meetup at EBLUG: Intro Talk and Demo</a>
      </li>
    
      <li class="post">
        <a href="/blog/blog/2014/11/17/ssh-without-trusting-bastion-host/">SSH Tip: Connecting to a Private Network Without Trusting the Bastion Host</a>
      </li>
    
      <li class="post">
        <a href="/blog/blog/2014/11/13/osv-private-beta/">OSv Early Access Private Beta</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  </div>
  <div class="footer">
    <div class="wrapf">


        <div class="custom"  >
            <p><a href="http://www.cloudius-systems.com"><img style="margin-top: 30px;" src="http://osv.io/images/footerLogo.jpg" alt="" width="380" height="38" /></a></p></div>



        <div class="custom social"  >
            <p><a href="https://plus.google.com/107787008629542080430" rel="publisher">Google+</a></p></div>



        <div class="custom CloudIcon"  >
            <p><a href="/index.php"><img src="http://osv.io/images/footerCloudIcon.jpg" width="57" height="33" alt="footerCloudIcon" /></a></p></div>

        <div class="clr"></div>
    </div>
</div>


  

<script type="text/javascript">
      var disqus_shortname = 'osvblog';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
